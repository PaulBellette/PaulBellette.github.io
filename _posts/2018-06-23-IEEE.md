---
layout: post
title: "Twice Ramanujan Sparsifiers"
date: 2018-06-23
---

So, this post is supposed to function as my notes for the upcoming IEEE signal processing journal club presentation. I decided to pick the excellent paper by Batson, Spielman and Srivastava on [Twice Ramanujan sparsifiers](https://arxiv.org/abs/0808.0163). I can't honestly even remember how I came across this paper, but it stuck in my mind as a really cool result that I wanted to understand one day. This seemed like as good a time as any to have a crack at it.

# So Why Should I Care About This?

So the dumbest and most misleading summary I can give would be that for a given undirected graph, there is a sparse graph that contains almost the same information. The paper gives a constructive proof that such a sparse graph exists by giving a polynomial time complexity algorithm for finding this approximating graph. Neat! Only problem is that it is cubic in the number of vertices, which is exactly the same complexity order as the problem I wanted to solve with it. Dang! Although there are other algorithms that do pretty much the same thing, with either randomized algorithms (ref) or less strong guarantees but less time complexity (ref). So as an utterly dirty applied mathematician I can use these and not feel too bad about it, because I know that such good approximating graphs actually do exist.

# A Motivating Example

I always find that a nice example from the everyday helps me understand why a particular result would be useful. So let's imagine that you had wasted the last six years of your life trying to find things in laser scans. A completely hypothetical situation. One of the big problems you have with measuring stuff in the wild is that absolute position is hard to get accurately (say with an RTK GNSS system, accuracy in the order of cm), whereas relative position measurements are pretty straight forward and high accuracy (a time of flight laser pulse system, accuracy in the order of mm). It would be nice to be able to use all the measurements together to simultaneously map the world and locate yourself to get a better estimate of where everything is than from the individual measurements alone.

So let's just pretend that you had a lovely pentagon that you wanted to measure, so you measured it from five positions around the outside with a terrestrial scanner, as represented in the following picture.

(pic here)

The set up is that the GNSS receiver on the scanner reports positions with a standard deviation of 5cm, and it measures the distance to the points of the pentagons with an accuracy of 5mm. Let's go crazy and say that it also measures where the other measurement positions are as well with the same accuracy (you left some markers on the ground that you can see with the scanner). Let's also make it completely fak and say there are no attitude errors as well. The measurements from the range finder from the ith point to the jth point give the following equations.

$$
\begin{align}
p_{i} - p_{j} = d_{i,j} + N(0,0.005) \\
p_{i} - p_{mi} = N(0,0.05) \\
\end{align}
$$

i.e. you measure the distance between two points with the Lidar, and the position of a point with the GNSS system (with different levels of noise, we can lie to ourselves and say it is Gaussian to make our lives easier). Let's make out lives even easier still and pretend we only care about the $z$ component of the position (generalizing this to all components is straight forward). Turning the usual crank of plugging into Bayes Theorem (with a few assumptions about priors) it can be shown that finding the $x$ that minimizes the following cost function will maximize the posterior probability of where are all the points given the data.

$$J = \lVert Ax - b\rVert_{2}^{2} + \lambda\lVert x - x_{0}\rVert$$

where $x$ is a vector of the $z$ position of all the measured points, $x_{0}$ is a vector of the GNSS measurements, $\lambda$ is a vector of regularization coefficients (zero for no GNSS measurement), $b$ is a vector where $b_i = \sum_{j=1}^nd_{ij}$ and (drum roll...) $A$ is the negative of the Graph Laplacian of the graph formed by considering the vertices of the graph as the positions to estimated and the presence of an edge indicates that a Lidar measurement between these two positions exist.

The awake amongst you notice that I have rigged the setup to give a complete graph here, 10 vertices too. Weird huh? It is a complete graph because every vertex is connected to every other vertex, as shown in th following picture.

(complete graph picture here)

Now this is all fine and dandy, we can solve this least squares problem with no issues at all. The question is what do w do when the problem gets freaking enormous, like the number of vertices is millions of feature point correspondences from thousands of poses in a large scale bundle adjustment problem? (Ignoring that the optimization problem is also non-linear now, let's imagine we are worried about linear approximations in an iterative scheme). You can a few approaches, exploit the structure of $A$ to decompose the problem into more reasonable sub problems, solve a smaller problem that gives you almost the same answer or design a really cool preconditioner for an low memory iterative solution scheme like the conjugate gradient method (provided you can get $A$ to fit in memory at all). The result of this paper can help with the last two approaches. I'll just show how for the first one.

#Dumbing it Down

So we probably want to try to get away with a shitload less edges than we have in our graph. The most aggressive thing you could do would be to try for a minimum spanning tree of the original graph. This will make it really fast to solve the optimization problem now, because your graph is super sparse. You might feel a little bad about yourself too, because you threw so much away. Let's try a couple of numerical experiments. First let's try running Prim's Minimum Spanning Tree Algorithm on the original complete graph and see what happens